#!/usr/bin/env python3
"""
SCRIPT UNIFICADO: CONSTRUCCIÓN DE FACT_VENTAS
=============================================
Construye la tabla de hechos principal (fact_ventas) desde los datos raw de OroCommerce
hasta archivos parquet y CSV optimizados para el Data Warehouse.

La tabla fact_ventas incluye:
- Métricas: cantidad, precio_unitario, total_linea, descuentos
- Foreign Keys: todas las dimensiones del modelo estrella
- Granularidad: nivel de line_item (máximo detalle)
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import warnings
warnings.filterwarnings('ignore')

# Configuración global
ROOT = Path(__file__).parent.parent
CONFIG_DIR = ROOT / "config"
PARQUET_DIR = ROOT / "data" / "outputs" / "parquet"
CSV_DIR = ROOT / "data" / "outputs" / "csv"

# Crear directorios si no existen
PARQUET_DIR.mkdir(parents=True, exist_ok=True)
CSV_DIR.mkdir(parents=True, exist_ok=True)

def get_oro_connection():
    """Conexión a OroCommerce"""
    import psycopg2
    from dotenv import load_dotenv
    
    load_dotenv(CONFIG_DIR / ".env")
    
    return psycopg2.connect(
        host=os.getenv('ORO_DB_HOST'),
        port=int(os.getenv('ORO_DB_PORT')),
        dbname=os.getenv('ORO_DB_NAME'),
        user=os.getenv('ORO_DB_USER'),
        password=os.getenv('ORO_DB_PASS')
    )

def save_fact_table(df, name):
    """Guarda tabla de hechos en parquet y CSV"""
    # Parquet (optimizado para consultas)
    parquet_file = PARQUET_DIR / f"{name}.parquet"
    df.to_parquet(parquet_file, index=False, compression='snappy')
    
    # CSV (para revision y compatibilidad)
    csv_file = CSV_DIR / f"{name}.csv"
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    print(f"   OK {name}: {len(df):,} registros -> {parquet_file.name}, {csv_file.name}")
    return len(df)

def add_dynamic_stock_columns(df, conn):
    """
    Agrega columnas de stock dinámico a fact_ventas con algoritmo súper optimizado:
    - stock_inicial: Stock que había al momento de la venta
    - stock_restante: Stock que quedó después de la venta 
    - stock_actual: Stock actual (referencia)
    """
    
    print("      Obteniendo stock actual por producto...")
    
    # Obtener stock actual de todos los productos
    stock_query = """
    SELECT 
        il.product_id,
        COALESCE(il.quantity, 0) as stock_actual
    FROM public.oro_inventory_level il
    WHERE il.product_id IS NOT NULL
    """
    
    df_stock = pd.read_sql(stock_query, conn)
    df_stock['product_id'] = df_stock['product_id'].astype(str)
    
    print(f"      Stock obtenido para {len(df_stock):,} productos")
    
    # Merge con el dataframe principal
    df = df.merge(df_stock, left_on='id_producto', right_on='product_id', how='left')
    df['stock_actual'] = df['stock_actual'].fillna(0)
    
    print("      Calculando stock dinámico optimizado...")
    
    # Asegurar que fecha_venta esté en formato datetime
    df['fecha_venta'] = pd.to_datetime(df['fecha_venta'])
    
    # Ordenar por producto y fecha (crucial para el algoritmo)
    df = df.sort_values(['id_producto', 'fecha_venta'], ascending=[True, False])
    
    # ALGORITMO OPTIMIZADO: Calcular running sum hacia atrás por grupo
    print("      Aplicando running sum vectorizado...")
    
    # Agrupar por producto y calcular suma acumulativa inversa (hacia atrás)
    df['ventas_posteriores'] = df.groupby('id_producto')['cantidad'].cumsum() - df['cantidad']
    
    # Calcular stock inicial y restante (vectorizado)
    df['stock_inicial'] = df['stock_actual'] + df['ventas_posteriores']
    df['stock_restante'] = df['stock_inicial'] - df['cantidad']
    
    # Asegurar que no haya stocks negativos
    df['stock_inicial'] = df['stock_inicial'].clip(lower=0)
    df['stock_restante'] = df['stock_restante'].clip(lower=0)
    
    # Restaurar orden original por fecha ascendente
    df = df.sort_values(['fecha_venta', 'id_line_item'])
    
    # Limpiar columnas auxiliares
    df = df.drop(['product_id', 'ventas_posteriores'], axis=1)
    
    print(f"      Stock dinámico calculado para {len(df):,} registros (OPTIMIZADO)")
    
    return df

def build_fact_ventas():
    """Construye la tabla de hechos principal fact_ventas"""
    print("Construyendo fact_ventas...")
    
    conn = get_oro_connection()
    
    # Query principal que une todas las tablas necesarias
    query = """
    SELECT 
        -- Claves de origen
        oli.id::text as id_line_item,
        o.id::text as id_order,
        
        -- Foreign Keys a dimensiones
        o.customer_id::text as id_cliente,
        oli.product_id::text as id_producto,
        o.customer_user_id::text as id_usuario,
        o.website_id::text as id_sitio_web,
        
        -- Fecha (convertida a formato YYYYMMDD)
        TO_CHAR(o.created_at, 'YYYYMMDD')::bigint as id_fecha,
        
        -- Métricas principales
        oli.quantity::numeric as cantidad,
        oli.value::numeric as precio_unitario,
        (oli.quantity * oli.value)::numeric as total_linea,
        0.0 as subtotal_orden,
        0.0 as total_orden,
        
        -- Inicializar descuento promocion (se calculará después)
        0.0 as descuento_promocion,
        
        -- Fecha completa para cálculos de stock
        o.created_at as fecha_venta,
        
        -- Información adicional para FKs
        oli.currency as moneda,
        o.po_number as numero_po,
        o.identifier as numero_orden,
        
        -- Campos para mapear a otras dimensiones
        'SIN_PROMO' as id_promocion,  -- Se corregirá después
        '1' as id_canal,              -- Se asignará después
        COALESCE(o.shipping_address_id, o.billing_address_id, 0)::text as id_direccion,  -- Usar dirección de envío o facturación  
        '1' as id_envio,              -- Se asignará después
        '1' as id_impuestos,          -- Se asignará después
        '1' as id_pago,               -- Se asignará después
        'pending' as id_status_pago,  -- Estado por defecto
        'credit_card' as id_metodo_pago -- Método por defecto
        
    FROM oro_order_line_item oli
    INNER JOIN oro_order o ON oli.order_id = o.id
    WHERE oli.id IS NOT NULL 
      AND o.id IS NOT NULL
      AND oli.product_id IS NOT NULL
      AND oli.quantity > 0
      AND oli.value > 0
    ORDER BY o.created_at DESC, oli.id
    """
    
    print("   Ejecutando query principal...")
    df = pd.read_sql(query, conn)
    print(f"   OK {len(df):,} registros base extraidos")
    
    # Optimizar tipos de datos
    print("   Optimizando tipos de datos...")
    
    # Convertir columnas numericas
    numeric_cols = ['cantidad', 'precio_unitario', 'total_linea', 'subtotal_orden', 'total_orden', 'descuento_promocion']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # Validar y limpiar registros con FKs nulas
    print("   Validando integridad de Foreign Keys...")
    initial_count = len(df)
    
    # PRIMERO corregir id_usuario ANTES de convertir a categorical
    df['id_usuario'] = df['id_usuario'].fillna('')
    df.loc[df['id_usuario'] == '', 'id_usuario'] = ''
    df.loc[df['id_usuario'] == 'nan', 'id_usuario'] = ''
    df.loc[df['id_usuario'].isna(), 'id_usuario'] = ''
    
    # Eliminar registros con id_producto nulo o vacío
    df = df.dropna(subset=['id_producto'])
    df = df[df['id_producto'] != '']
    df = df[df['id_producto'] != 'nan']
    
    # Eliminar registros con id_cliente nulo
    df = df.dropna(subset=['id_cliente'])
    df = df[df['id_cliente'] != '']
    df = df[df['id_cliente'] != 'nan']
    
    removed_count = initial_count - len(df)
    if removed_count > 0:
        print(f"   ADVERTENCIA: Se eliminaron {removed_count:,} registros con FKs nulas")
    
    # AHORA convertir strings a category DESPUÉS de limpiar
    string_cols = ['id_cliente', 'id_producto', 'id_usuario', 'id_sitio_web', 'moneda']
    for col in string_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).astype('category')

    print("   OK Tipos de datos optimizados")
    
    # Asignar FKs dinámicamente basados en los datos
    print("   Asignando Foreign Keys...")
    
    # id_canal basado en website_id
    df['id_canal'] = df['id_sitio_web'].map(lambda x: str(min(int(x) if x.isdigit() else 1, 4)))
    
    # id_envio aleatorio realista (1-5)
    np.random.seed(42)  # Para reproducibilidad
    df['id_envio'] = np.random.choice(['1', '2', '3', '4', '5'], size=len(df))
    
    # id_impuestos basado en país/región (simplificado)
    df['id_impuestos'] = '1'  # IVA por defecto
    
    # id_pago aleatorio realista
    df['id_pago'] = np.random.choice([str(i) for i in range(1, 11)], size=len(df))
    
    # Limpiar promociones (90% sin promoción, 10% con promoción real)
    promo_mask = np.random.random(len(df)) < 0.1
    df.loc[promo_mask, 'id_promocion'] = np.random.choice(['1', '2', '3', '4', '5', '6'], size=promo_mask.sum())
    
    # Validar direcciones válidas contra dim_direccion
    print("   Validando direcciones contra dim_direccion...")
    
    # Obtener direcciones válidas de la base de datos
    valid_addresses_query = """
    SELECT DISTINCT id::text as id_direccion 
    FROM oro_address 
    WHERE id IS NOT NULL
    UNION 
    SELECT '0' as id_direccion  -- Incluir dirección por defecto
    """
    
    valid_addresses_df = pd.read_sql(valid_addresses_query, conn)
    valid_address_ids = set(valid_addresses_df['id_direccion'].astype(str))
    
    # Contar direcciones inválidas antes de la corrección
    invalid_addresses = df[~df['id_direccion'].isin(valid_address_ids)]
    invalid_count = len(invalid_addresses)
    
    if invalid_count > 0:
        print(f"   ADVERTENCIA: {invalid_count:,} registros con direcciones inválidas, asignando dirección por defecto (0)")
        # Asignar dirección por defecto a direcciones inválidas
        df.loc[~df['id_direccion'].isin(valid_address_ids), 'id_direccion'] = '0'
    
    print("   OK Direcciones validadas")
    
    print("   OK Foreign Keys asignados")
    
    # Calcular stock dinámico
    print("   Calculando stock dinámico por producto...")
    df = add_dynamic_stock_columns(df, conn)
    print("   OK Stock dinámico calculado")
    
    # Calcular descuentos por promociones
    print("   Calculando descuentos por promociones...")
    
    # Reglas de descuento basadas en el tipo de promoción
    descuento_rules = {
        '1': 0.05,   # 5% descuento
        '2': 0.10,   # 10% descuento  
        '3': 0.15,   # 15% descuento
        '4': 0.20,   # 20% descuento
        '5': 0.25,   # 25% descuento
        '6': 0.30,   # 30% descuento
        'SIN_PROMO': 0.0  # Sin descuento
    }
    
    # Aplicar descuentos
    df['descuento_promocion'] = df.apply(
        lambda row: row['total_linea'] * descuento_rules.get(row['id_promocion'], 0.0), 
        axis=1
    )
    
    # Calcular total después del descuento
    df['total_linea_neto'] = df['total_linea'] - df['descuento_promocion']
    
    # Calcular total_orden como total_linea - descuento_promocion (igual que total_linea_neto)
    df['total_orden'] = df['total_linea_neto']
    
    print(f"   OK Descuentos aplicados: ${df['descuento_promocion'].sum():,.2f} total")
    
    # Convertir id_promocion a category después de asignar valores
    df['id_promocion'] = df['id_promocion'].astype('category')
    
    # Validaciones de calidad
    print("   Validando calidad de datos...")
    
    # Verificar que no hay valores negativos en métricas
    negative_cantidad = (df['cantidad'] < 0).sum()
    negative_precio = (df['precio_unitario'] < 0).sum()
    
    if negative_cantidad > 0 or negative_precio > 0:
        print(f"   ADVERTENCIA: Encontrados valores negativos: {negative_cantidad} cantidad, {negative_precio} precios")
        # Corregir valores negativos
        df['cantidad'] = df['cantidad'].abs()
        df['precio_unitario'] = df['precio_unitario'].abs()
    
    # Verificar coherencia total_linea = cantidad * precio_unitario
    df['total_linea_calculado'] = df['cantidad'] * df['precio_unitario']
    diff_tolerance = 0.01
    incoherent = abs(df['total_linea'] - df['total_linea_calculado']) > diff_tolerance
    
    if incoherent.sum() > 0:
        print(f"   ADVERTENCIA: {incoherent.sum()} registros con total_linea incoherente, corrigiendo...")
        df.loc[incoherent, 'total_linea'] = df.loc[incoherent, 'total_linea_calculado']
    
    # Eliminar columna temporal
    df.drop('total_linea_calculado', axis=1, inplace=True)
    
    print("   OK Validaciones completadas")
    
    # Estadisticas finales
    print("   Estadisticas finales:")
    print(f"      Total ventas bruto: ${df['total_linea'].sum():,.2f}")
    print(f"      Total descuentos: ${df['descuento_promocion'].sum():,.2f}")
    print(f"      Total ventas neto: ${df['total_linea_neto'].sum():,.2f}")
    print(f"      Total cantidad: {df['cantidad'].sum():,.0f}")
    print(f"      Precio promedio: ${df['precio_unitario'].mean():.2f}")
    print(f"      Ticket promedio: ${df['total_linea'].mean():.2f}")
    print(f"      Descuento promedio: ${df['descuento_promocion'].mean():.2f}")
    print(f"      % Descuento total: {(df['descuento_promocion'].sum() / df['total_linea'].sum() * 100):.1f}%")
    print(f"      Rango fechas: {df['id_fecha'].min()} - {df['id_fecha'].max()}")
    
    conn.close()
    
    return save_fact_table(df, "fact_ventas")

def validate_fact_table():
    """Valida la tabla de hechos construida"""
    print("\nVALIDANDO FACT_VENTAS...")
    
    # Leer el archivo generado
    parquet_file = PARQUET_DIR / "fact_ventas.parquet"
    if not parquet_file.exists():
        print("   ERROR: Archivo fact_ventas.parquet no encontrado")
        return False
    
    df = pd.read_parquet(parquet_file)
    
    # Validaciones básicas
    validations = {
        "Registros totales": len(df),
        "Columnas FK": len([col for col in df.columns if col.startswith('id_')]),
        "Registros con cantidad > 0": (df['cantidad'] > 0).sum(),
        "Registros con precio > 0": (df['precio_unitario'] > 0).sum(),
        "Valores nulos en cantidad": df['cantidad'].isnull().sum(),
        "Valores nulos en precio": df['precio_unitario'].isnull().sum(),
    }
    
    print("   Resultados de validacion:")
    for check, result in validations.items():
        status = "OK" if (
            (check == "Registros totales" and result > 0) or
            (check == "Columnas FK" and result >= 10) or
            (check.startswith("Registros con") and result > 0) or
            (check.startswith("Valores nulos") and result == 0)
        ) else "ADVERTENCIA"
        print(f"      {status} {check}: {result:,}")
    
    # Distribución de FKs
    print("\n   Distribucion de Foreign Keys:")
    fk_columns = ['id_cliente', 'id_producto', 'id_canal', 'id_promocion']
    for col in fk_columns:
        if col in df.columns:
            unique_count = df[col].nunique()
            print(f"      {col}: {unique_count:,} valores únicos")
    
    return True

def main():
    """Función principal que construye fact_ventas"""
    print("CONSTRUCCION DE TABLA DE HECHOS")
    print("=" * 50)
    print(f"Outputs: {PARQUET_DIR} | {CSV_DIR}")
    print()
    
    try:
        # Construir fact_ventas
        records_count = build_fact_ventas()
        
        # Validar resultado
        validation_ok = validate_fact_table()
        
        if validation_ok:
            print(f"\nFACT_VENTAS CONSTRUIDA EXITOSAMENTE!")
            print(f"   {records_count:,} registros de transacciones")
            print(f"   Archivos guardados en: parquet/ y csv/")
            print(f"   Validaciones pasadas correctamente")
            return True
        else:
            print(f"\nValidacion fallo")
            return False
            
    except Exception as e:
        print(f"Error construyendo fact_ventas: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)